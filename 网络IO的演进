    最近学了一下网络IO的演进过程,做个记录,加深印象.
    
    我们先来了解一下linux系统IO的一些知识.linux系统一切皆文件，当服务端接收一个连接(socket)，就会为此连接分配一个文件描述符(file descriptor),后续的该连接的IO事件都通过文件描述符进行对应.

    先来看看最早的网络IO方式

/**
 * @author gongxiufu
 * @date 2021/5/18 22:31
 */
public class BioServer {
    public static void main(String[] args) throws IOException {
        //监听8080端口
        ServerSocket serverSocket = new ServerSocket(8080);
        while(true) {
            Socket accept = serverSocket.accept();
            InputStream inputStream = accept.getInputStream();
            byte[] buff = new byte[1024];
            int len;
            //read 方法是阻塞的，当没有数据到来时,线程会一直阻塞在read方法
            while ((len = inputStream.read(buff)) != -1) {
                System.out.println(new String(buff,0,len));
            }
        }
    }
}

    服务端开启一个线程监听连接事件,当有连接过来时,接收连接,然后监听数据的到来,如果没有数据线程则一直处于阻塞状态,不能做其他的事情.所以早期的服务器设计中,为了能够同时服务多个连接,通常采用的方式是
服务端每accept一个连接,就开启一个新的线程来监听将要到达的数据.

    此时服务端读取数据的IO模型：用户空间向DMI磁盘控制器发出读取请求---DMI磁盘控制器从内核空间复制数据到缓存区---缓冲区复制数据到用户空间,存在用户空间与内核空间的切换,存在两次内存拷贝
    
    这个模式最主要的缺点就是阻塞IO,每当有一个新的客户端来连接服务端时，我们都需要为这个客户端创建一个线程来处理读数据的操作，当并发度很高时，我们就需要创建很多的线程，这显然这是不可取的。线程是服务器的宝贵资
源，创建和销毁都需要花费很长时间，当线程过多时，CPU的上线文切换也更加频繁，这样就会造成服务响应缓慢。当线程过多时，甚至还会出现句柄溢出、OOM等异常，最终导致服务宕机.后来通过线程池的方式来解决
线程频繁创建的问题，但是依然无法解决内核阻塞read的缺陷.
    因此BIO的服务端，适用于并发度不高的应用场景，但是对于高并发，服务负载较重的场景，使用BIO显然是不适合的。
    
    我们要记住,IO方式的改变最主要的原因是内核改变！！！！
    
    接下来，我们迎来了NIO，意思为非阻塞IO，NIO只针对网络IO有意义.我们知道IO方式的改变，肯定是内核发生了变化.linux 为我们提供了一种新的IO方式,线程可以不再阻塞一直等待 accept、read 等IO事件到来,它可以循环的检查所有的连接，
 如果检查到有IO事件，则进行相应的IO操作，没有则检查下一个连接的IO事件.
 
    /**
 * @author gongxiufu
 * @date 2021/5/18 23:04
 */
public class NioServer {
    public static void main(String[] args) throws IOException {
        //打开一个ServerSocketChannel，作用是accept
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();
        //监听端口
        serverSocketChannel.bind(new InetSocketAddress(8080));
        //设置为非阻塞
        serverSocketChannel.configureBlocking(false);
        // Selector 选择器
        Selector selector = Selector.open();
        //注册感兴趣事件
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);

        while(true) {
            //一直阻塞直到有IO事件
            Set<SelectionKey> selectionKeys = selector.selectedKeys();
            Iterator<SelectionKey> iterator = selectionKeys.iterator();
            while (iterator.hasNext()) {
                SelectionKey selectionKey = iterator.next();
                if(selectionKey.isAcceptable()) {
                    SocketChannel socketChannel = serverSocketChannel.accept();
                    //设置为非阻塞
                    socketChannel.configureBlocking(false);
                    //注册读感兴趣事件
                    socketChannel.register(selector,SelectionKey.OP_READ);
                }
                if(selectionKey.isReadable()) {
                    SocketChannel  channel = (SocketChannel)selectionKey.channel();
                    ByteBuffer byteBuffer = ByteBuffer.allocate(1024);
                    channel.read(byteBuffer);
                    //切换写模式
                    byteBuffer.flip();
                    System.out.println(Charset.defaultCharset().decode(byteBuffer));
                    //再次注册读感兴趣事件
                    channel.register(selector,SelectionKey.OP_READ);
                }
                // 将SelectionKey从集合中移除，
                // 这一步很重要，如果不移除，那么下次调用selectKeys()方法时，又会遍历到该SelectionKey，这就造成重复处理了，而且最终selectionKeys这个集合的大小会越来越大。
                iterator.remove();
            }
        }
    }
}


    此时的IO模型：和BIO一样,还是要经历用户空间到内核空间的数据读取过程,但是线程无需一直等待某个连接的IO事件,而是可以对所有连接进行IO事件轮询
    
    这个模式比起NIO,线程阻塞的缺陷已经得到了解决，但是每一次轮询,都存在用户空间和内核空间切换的问题,这会产生大量的用户空间和内核空间的切换.
    
    
    我们要记住,IO方式的改变最主要的原因是内核改变！！！！
    
        NIO的每次轮询,只检测一个文件描述符的IO就绪事件，linux内核对这种方式进行了改进，支持用户系统提供一批文件描述符，并且检测是否有IO事件的工作由linux内核完成的selector 模型.用户系统提供一批文件描述符给内核，
    进行检测,内核检查之后，把IO就绪的情况返回给用户系统,然后用户系统对就绪的IO事件进行读取,没有IO就绪的连接则省去了用户空间和内核空间的交互.
        
        select 模型比起NIO，避免了没有IO事件就绪的无效用户空间和内核空间的切换,却依然存在痛点.比如早期文件描述符限制,一次传递大量的文件描述符,内核检查IO事件却没有就绪的无效查询,系统的性能严重浪费在检查IO事件上，然而我们
    想要的是集中系统资源服务于用户相关的内容,减少无效的系统消耗.   
    

    我们要记住,IO方式的改变最主要的原因是内核改变！！！！
    
        最后就是我们现在最流行的的epoll模型,linux 通过 epoll_create 创建用于epoll IO模式的文件描述符,当服务器接收到一个连接时，则把该连接的文件描述符通过 epoll_ctl 保存到 epoll_create 创建的
    文件描述符中,epoll_create 创建的文件描述符又分为两部分，一部分存储所有的连接描述符，当IO时间就绪时，则把该描述符放置到另外一个部分，最后用户系统通过 epoll_wait 调用系统内核,内核直接返回
    就绪的IO事件，这就是epoll IO 模型.
    
        epoll 对比 select ,文件描述符的维护工作由内核完成,不需要传递大量的文件描述符,对就绪的IO事件进行存储，每次调用系统只返回就绪的IO事件，极大的提升了IO性能.
        
        
    总结：
        1.IO事件的变化主要有系统内核决定
        2.演进优化方式,阻塞变为非阻塞,单一变批量,被动式变为响应式
       


     

 
